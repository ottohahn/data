{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Language classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Characters used in a language have a direct correlation with the language itself. Design a solution that will allow a user to provide a document and identify (classify) the language it was written on. The languages that this tool should identify are: Spanish, English, Italian, German, French, Portuguese, and Danish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia as wk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import unicodedata\n",
    "import string\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Acquisition\n",
    "Let's begin by getting the text from the various wikipedia sites provides. We'll use this for both training and testing our model. We will be using the \"Wikipedia\" package in python to get our data. For inputs, the functions in this package require languages and page names, which we can grab directly from the links provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages_s = ['es','it','en','de','fr','pt','nl']\n",
    "languages_l = ['Spanish','Italian','English','German','French','Portuguese','Danish']\n",
    "page_names  = ['Divina_comedia', \n",
    "               'Divina_Commedia', \n",
    "               'Divine_Comedy',\n",
    "               'Göttliche_Komödie', \n",
    "               'Divine_Comédie', \n",
    "               'Divina_Comédia',\n",
    "               'De_goddelijke_komedie']\n",
    "text_splits = ['== Traducciones ==', \n",
    "               '== Data di composizione ==',\n",
    "               '== Earliest manuscripts ==', \n",
    "               '== Rezeptionsgeschichte ==', \n",
    "               'Sainte Vierge et finalement Dante',\n",
    "               'Virgem Maria e esta concede sua visita', \n",
    "               '== Receptie ==']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text splits are necessary because generally towards the end of Wikipedia pages there are references and glossaries, which while useful, do not contain full sentences. It's not a requirement for documents (sentences) to always be complete but I would like to be consistent with the data for this exercise. Also note that not all the text splits occur at a section heading, python can't always deal with unicode correctly so in some cases I split the data at an earlier point.\n",
    "\n",
    "**Please note that when I am referring to documents, I am referring to a sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['== Traducciones ==',\n",
       " '== Data di composizione ==',\n",
       " '== Earliest manuscripts ==',\n",
       " '== Rezeptionsgeschichte ==',\n",
       " 'Sainte Vierge et finalement Dante',\n",
       " 'Virgem Maria e esta concede sua visita',\n",
       " '== Receptie ==']"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a list of the languages that we will be classifiying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "español\n",
      "italiano\n",
      "English\n",
      "Deutsch\n",
      "français\n",
      "português\n",
      "Nederlands\n"
     ]
    }
   ],
   "source": [
    "for language in languages_s:\n",
    "    print wk.languages()[language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Danish': 7,\n",
       " 'English': 3,\n",
       " 'French': 5,\n",
       " 'German': 4,\n",
       " 'Italian': 2,\n",
       " 'Portuguese': 6,\n",
       " 'Spanish': 1}"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_dict = dict(zip(languages_l, [1,2,3,4,5,6,7]))\n",
    "rev_dict = dict(zip([1,2,3,4,5,6,7], languages_l))\n",
    "lang_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary of all the language content from each of the wikipedia pages, with languages as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki(language, page_name):\n",
    "    wk.set_lang(language)\n",
    "    data = wk.page(page_name)\n",
    "    return data.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = {}\n",
    "for row in zip(languages_s, languages_l, page_names, text_splits):\n",
    "    content[row[1]] = (get_wiki(row[0], row[2])).split(row[3])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data in a format that will be acceptable for the model, we will have to transform it in the following ways:\n",
    "* convert unicode to ascii\n",
    "* split into sentences\n",
    "* remove punctuation\n",
    "* lowercase all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_df(languages, content, int_conv):\n",
    "    dfs = []\n",
    "    idx = 0\n",
    "    for l in languages:\n",
    "        sent = unicodedata.normalize('NFKC', content[l]).encode('ascii', 'ignore')\n",
    "        sent = nltk.sent_tokenize(sent, language=l)\n",
    "        sent = [(x.translate(string.maketrans(\"\",\"\"), string.punctuation)).lower() for x in sent]\n",
    "        rge = np.arange(idx, idx+len(sent), 1)\n",
    "        text= pd.Series(data=sent, name='Text', index=rge)\n",
    "        label = pd.Series(data=[int_conv[l] for x in sent], name='Label', index=rge)\n",
    "        dfs.append(pd.concat([text, label], axis=1))\n",
    "        idx = idx + 1 + len(sent)\n",
    "    return pd.concat([x for x in dfs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>la divina comedia en italiano divina commedia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es un poema escrito por dante alighieri</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>se desconoce la fecha exacta en que fue escrit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>libro ms famoso de su autor es una de las obra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>es considerada la obra maestra de la literatur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0      la divina comedia en italiano divina commedia      1\n",
       "1            es un poema escrito por dante alighieri      1\n",
       "2  se desconoce la fecha exacta en que fue escrit...      1\n",
       "3  libro ms famoso de su autor es una de las obra...      1\n",
       "4  es considerada la obra maestra de la literatur...      1"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = transform_df(languages_l, content, lang_dict)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe of all the text from the various wikipedia pages converted into sentences and labels based on language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    361\n",
       "5    277\n",
       "4    218\n",
       "2    113\n",
       "1    106\n",
       "6     88\n",
       "3     68\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is definitely not uniformly distributed between the languages. Since individuals contribute to wikipedia articles and there is no required consistency of text between languages, it looks like the same page can vary widely between languages. This can be adjusted for when using the Multinomial Naive Bayes classifier in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation, Training, and Testing\n",
    "One of the most popular text classifiers is the Multinomial Naive Bayes classifier. We can try a random forest classifier as well for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can't pass a dataframe of text directly to a classifier, let's convert our modified data into a vector - two methods:\n",
    "* Count Vector: For each document(sentence), count the number of time each word appears (term frequency)\n",
    "* TFIDF Vector: term frequency * inverse document frequency\n",
    "\n",
    "See link for more details on TFIDF: https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_df=0.95)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(result['Text'].values)\n",
    "count = count_vectorizer.fit_transform(result['Text'].values)\n",
    "mnb = MultinomialNB(alpha=0.1, fit_prior=False)\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_test(data, model, t_size, data_type, model_type):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, \\\n",
    "        result['Label'].values, test_size = t_size, random_state=8)\n",
    "    model.fit(X_train, y_train)\n",
    "    cv_score = np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy'))\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    print \"The cross-validation score for the %s model, using %s data is %s\" \\\n",
    "        % (model_type, data_type, cv_score)\n",
    "    print \"The test set score for the %s model, using %s data is %s\" \\\n",
    "        % (model_type, data_type, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validation score for the Multinomial NB model, using TFIDF data is 0.989221534899\n",
      "The test set score for the Multinomial NB model, using TFIDF data is 0.977272727273\n"
     ]
    }
   ],
   "source": [
    "model_test(tfidf, mnb, .25, \"TFIDF\", \"Multinomial NB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validation score for the Multinomial NB model, using Count Vectorizer data is 0.988134578378\n",
      "The test set score for the Multinomial NB model, using Count Vectorizer data is 0.980519480519\n"
     ]
    }
   ],
   "source": [
    "model_test(count, mnb, .25, \"Count Vectorizer\", \"Multinomial NB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validation score for the Random Forest model, using TFIDF data is 0.946887706354\n",
      "The test set score for the Random Forest model, using TFIDF data is 0.957792207792\n"
     ]
    }
   ],
   "source": [
    "model_test(tfidf, rf, .25, \"TFIDF\", \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validation score for the Random Forest model, using Count Vectorizer data is 0.943626008017\n",
      "The test set score for the Random Forest model, using Count Vectorizer data is 0.941558441558\n"
     ]
    }
   ],
   "source": [
    "model_test(count, rf, .25, \"Count Vectorizer\", \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multinomial Naive Bayes (MNB) model using TFIDF data performed the best, although not much better than the same model using Count Vector data. Either one could be used to do further predictions on other data. The Random Forest models didn't perform poorly either, they just can't match the near perfect performance of MNB. Since the test data used for the above scores also came from the same source - Divine Comedy (religious text), it would be interesting to see how the best model above performs on data from a different source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a final model using all the training data, which I can then use on new text strings to predict what language they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = mnb.fit(tfidf, result['Label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Outside Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model I used above requires that an array of multiple values be passed as test data for prediction. Let's get text from different Wikipedia sites for each of the languages. All of the below strings were copied directly from Wikipedia and pasted below. ** None of the strings below come from the Divine Comedy page.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt1 = \"A peer review of 42 science articles found in both Encyclopædia Britannica and Wikipedia was published in Nature in 2005, and found that Wikipedia's level of accuracy approaches Encyclopedia Britannica's\"\n",
    "lang1 = \"English\"\n",
    "txt2 = \"A pesar de que el bajo imperio se extendía por las tierras de la periferia del Mediterráneo, en la historia militar de Roma las batallas navales fueron, por lo general, menos significativas que las batallas terrestres, debido a su dominio casi incuestionable del mar tras las feroces luchas navales de la Primera Guerra Púnica.\"\n",
    "lang2 = \"Spanish\"\n",
    "txt3 = \"Dal 1951 al 1956 con la compagine biancorossa vinse numerosi trofei e disputò la finale di Coppa dei Campioni contro il Real Madrid nell'edizione 1955-1956; qualche settimana dopo si trasferì proprio alla squadra spagnola. Giocò tre stagioni a Madrid in cui vinse tre volte la Coppa dei Campioni, due volte il campionato nazionale e una Coppa Latina.\"\n",
    "lang3 = \"Italian\"\n",
    "txt4 = \"Son règne est marqué par une double fidélité : à l'Empire, dont il tire sa légitimité en tant que vicaire impérial ; au parti gibelin, dont il devient le chef incontesté dans le nord de l'Italie.\"\n",
    "lang4 = \"French\"\n",
    "txt5 = \"Den ihr allgemein gegebenen Namen der „großen Gräfin“ verdankt sie ebenso ihrer Macht wie ihren glänzenden Geistesgaben und ihrer hohen Bildung.\"\n",
    "lang5 = \"German\"\n",
    "txt6 = \"Ao final da Guerra de Sucessão Espanhola com o Tratado de Utrecht em 1713, o duque de Savoia readquiriu suas possessões originais e recebeu o título de Rei da Sicília.\"\n",
    "lang6 = \"Portuguese\"\n",
    "txt7 = \"Sinds de nieuwe tijd, toen de Europeanen met de verkenning en onderwerping van de rest van de wereld begonnen werd de westerse cultuur de dominante cultuur van de wereld.\"\n",
    "lang7 = \"Danish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_txt = [txt1, txt2, txt3, txt4, txt5, txt6, txt7]\n",
    "test_lang = [lang1, lang2, lang3, lang4, lang5, lang6, lang7]\n",
    "test_txt_cln = []\n",
    "for txt in test_txt:\n",
    "    source = unicode(txt, 'utf-8')\n",
    "    sent = unicodedata.normalize('NFKC', source).encode('ascii', 'ignore')\n",
    "    sent = sent.translate(string.maketrans(\"\",\"\"), string.punctuation).lower()\n",
    "    test_txt_cln.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_txt_cln = np.array(test_txt_cln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our existing model and create predictions for the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = tfidf_vectorizer.transform(test_txt_cln)\n",
    "pred = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: English\n",
      "Actual: English\n",
      "_____\n",
      "Prediction: Spanish\n",
      "Actual: Spanish\n",
      "_____\n",
      "Prediction: Italian\n",
      "Actual: Italian\n",
      "_____\n",
      "Prediction: French\n",
      "Actual: French\n",
      "_____\n",
      "Prediction: German\n",
      "Actual: German\n",
      "_____\n",
      "Prediction: Portuguese\n",
      "Actual: Portuguese\n",
      "_____\n",
      "Prediction: Danish\n",
      "Actual: Danish\n",
      "_____\n"
     ]
    }
   ],
   "source": [
    "pred_l = []\n",
    "for val in pred:\n",
    "    pred_l.append(rev_dict[val])\n",
    "\n",
    "for val in zip(pred_l, test_lang):\n",
    "    print \"Prediction: %s\" % val[0]\n",
    "    print \"Actual: %s\" % val[1]\n",
    "    print \"_____\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model did well on random data from other Wikipedia pages, granted there were only 7 test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Final Thoughts\n",
    "I was able to create a model that classified text strings into one of seven languages pretty accurately. The nature of language is that there are far fewer common words/spellings between languages than unique ones. For this reason, the classifier that I built was able to parse through text strings and generally have a high degree of classification success based on the training data.\n",
    "\n",
    "As stated earlier, the Multinomial Naive Bayes model had the highest degree of success. Using count vectors or TFIDF did not lead to much difference in classification accuracy.\n",
    "\n",
    "I used 5-fold cross validation on the training data to check for overfitting, the models did not overfit as the test scores were very similar to the cross validation scores.\n",
    "\n",
    "The model above was also able to generalize pretty well on unseen data (granted I only used 7 data points for this portion of the exercise).\n",
    "\n",
    "If I had all the time in the world, I would do the following to improve my solution:\n",
    "* Train the model on several articles, I only had about 1200 documents (sentences), which is far fewer than what was used in some papers I read. Having more articles would increase the overall vocabulary of each language significantly.\n",
    "* Consider using a stop-word list for each language. I didn't use stop-words because I didn't find a list for every language. Removing stop words could be useful since some short words look the same in multiple languages. This also may not have helped but it would have been interesting to check.\n",
    "* Create a one-off language classification check. The sklearn implementation required an array of predictions (greater than 1). It would have been nice to create a function that could take any random single string and create a prediction.\n",
    "* Create a large test set from other sources and test the model again.\n",
    "* I had issues with unicode to ascii conversion when \"Testing on Outside Data\". I would research this issue more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "* https://github.com/goldsmith/Wikipedia - Wikipedia API wrapper for Python\n",
    "* https://en.wikipedia.org/wiki/Tf%E2%80%93idf - Explanation of TFIDF\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html - Scikit Learn Multinomial Naive Bayes implementation\n",
    "* http://corporavm.uni-koeln.de/vardial/sharedtask.html - DSL Shared Task problem set\n",
    "* O'Reilly, Natural Language Processing with Python, 2009\n",
    "* As always, several stack overflow pages were also of use when I got stuck"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
